import os
import pathlib
import time
import random
import re
from datetime import datetime
import requests

import cloudscraper
from bs4 import BeautifulSoup

# Playwright
from playwright.sync_api import sync_playwright

# ç›£è¦–å¯¾è±¡URLãƒªã‚¹ãƒˆ
PRODUCT_URLS = [
    "https://www.popmart.com/jp/products/3889/THE-MONSTERS-%E3%82%B3%E3%82%AB%E3%83%BB%E3%82%B3%E3%83%BC%E3%83%A9-%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%E3%81%AC%E3%81%84%E3%81%90%E3%82%8B%E3%81%BF",
    "https://www.popmart.com/jp/products/5771/DIMOO-Shapes-in-Nature-%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA"
]

WEBHOOK_URL = os.environ.get("DISCORD_WEBHOOK_URL")
STATUS_DIR = "statuses"
pathlib.Path(STATUS_DIR).mkdir(exist_ok=True)

def safe_filename(name):
    return re.sub(r'[^0-9a-zA-Z_-]', '_', name)

def fetch_page_cloudscraper(url):
    """è»½é‡ãƒã‚§ãƒƒã‚¯ç”¨ Cloudscraper å–å¾—"""
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
    )
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/124.0 Safari/537.36",
        "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7"
    }
    res = scraper.get(url, headers=headers)
    return res.text

def fetch_page_playwright(url):
    """JSãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å¾Œã®HTMLå–å¾—"""
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)  # Chromium ã‚’æ˜ç¤ºçš„ã«æŒ‡å®š
        page = browser.new_page()
        page.goto(url)
        page.wait_for_timeout(2000)  # JSæç”»å¾…ã¡
        html = page.content()
        browser.close()
    return html

def parse_product_info(html):
    """å•†å“åã€ç”»åƒURLã€åœ¨åº«åˆ¤å®š"""
    soup = BeautifulSoup(html, "html.parser")
    
    # å•†å“å
    product_name_tag = soup.find("h1", class_=re.compile("ProductDetail_title"))
    product_name = product_name_tag.get_text(strip=True) if product_name_tag else None

    # OGç”»åƒ
    og_img = soup.find("meta", property="og:image")
    image_url = og_img["content"] if og_img else None

    # åœ¨åº«åˆ¤å®š
    text_clean = re.sub(r'\s+', '', soup.get_text())
    if "ã‚«ãƒ¼ãƒˆã«è¿½åŠ ã™ã‚‹" in text_clean or "ä»Šã™ãè³¼å…¥" in text_clean:
        status = "in_stock"
    elif "å†å…¥è·ã‚’é€šçŸ¥ã™ã‚‹" in text_clean:
        status = "out_of_stock"
    else:
        status = "unknown"

    return product_name, image_url, status

def load_last_status(product_name):
    file_path = pathlib.Path(STATUS_DIR) / f"{safe_filename(product_name)}.txt"
    if file_path.exists():
        return file_path.read_text().strip()
    return None

def save_last_status(product_name, status):
    file_path = pathlib.Path(STATUS_DIR) / f"{safe_filename(product_name)}.txt"
    file_path.write_text(status)

def notify_discord(product_name, status, url, image_url=None):
    color = 0x00ff00 if status == "in_stock" else 0xff0000
    status_text = "âœ… åœ¨åº«ã‚ã‚Š" if status == "in_stock" else "âŒ åœ¨åº«ãªã—" if status == "out_of_stock" else "â“ åˆ¤å®šä¸å¯"
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

    embed = {
        "title": f"{product_name} åœ¨åº«å¤‰åŒ–æ¤œçŸ¥",
        "description": f"{status_text}\n[å•†å“ãƒšãƒ¼ã‚¸ã¯ã“ã¡ã‚‰]({url})\nğŸ•’ {timestamp}",
        "color": color
    }
    if image_url:
        embed["thumbnail"] = {"url": image_url}

    payload = {"embeds": [embed]}
    try:
        requests.post(WEBHOOK_URL, json=payload, timeout=10)
    except Exception as e:
        print(f"âŒ Discordé€šçŸ¥é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}")

def main():
    print("ğŸš€ POPMARTç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’èµ·å‹•ã—ã¾ã—ãŸ")
    while True:
        for url in PRODUCT_URLS:
            try:
                # 1. Cloudscraperã§è»½é‡ãƒã‚§ãƒƒã‚¯
                html = fetch_page_cloudscraper(url)
                product_name, image_url, status = parse_product_info(html)

                # 2. åˆ¤å®šã§ããªã„å ´åˆã¯ Playwright ã§è©³ç´°ãƒã‚§ãƒƒã‚¯
                if not product_name or status == "unknown":
                    html = fetch_page_playwright(url)
                    product_name, image_url, status = parse_product_info(html)
            except Exception as e:
                print(f"âŒ {url} å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                continue

            last_status = load_last_status(product_name)

            # åˆå›ã¯ä¿å­˜ã®ã¿
            if last_status is None:
                save_last_status(product_name, status)
                print(f"åˆå›åˆ¤å®š: {product_name} ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ä¿å­˜ {status}")
                continue

            # åœ¨åº«å¤‰åŒ–æ™‚ã®ã¿é€šçŸ¥
            if status != last_status:
                notify_discord(product_name, status, url, image_url)
                save_last_status(product_name, status)
                print(f"ğŸ”” {product_name} åœ¨åº«å¤‰åŒ–: {last_status} â†’ {status}")
            else:
                print(f"{product_name} ã®åœ¨åº«å¤‰åŒ–ãªã— ({status})")

        # æ¬¡ã®ãƒã‚§ãƒƒã‚¯ã¾ã§ãƒ©ãƒ³ãƒ€ãƒ ã‚¹ãƒªãƒ¼ãƒ— 25ã€œ45ç§’
        sleep_time = random.randint(25, 45)
        print(f"æ¬¡ã®ãƒã‚§ãƒƒã‚¯ã¾ã§ {sleep_time} ç§’å¾…æ©Ÿ...")
        time.sleep(sleep_time)

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"âŒ ã‚¹ã‚¯ãƒªãƒ—ãƒˆèµ·å‹•ã‚¨ãƒ©ãƒ¼: {e}")
